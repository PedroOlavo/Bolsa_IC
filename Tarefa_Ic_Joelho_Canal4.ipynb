{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarefa_Ic_Joelho_Canal4.ipynb",
      "provenance": [],
      "mount_file_id": "17lP4oWFR-c4tmTg_aqKqPOHm3eTRSB8W",
      "authorship_tag": "ABX9TyPDg7h+YqGrwJVcgDLnUgQW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroOlavo/Bolsa_IC/blob/main/Tarefa_Ic_Joelho_Canal4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0LY1mt36RwwK"
      },
      "source": [
        "Teste de rede neural com o banco que criei a partir do canal 4, foram usadas 4 m√©tricas para avaliar se o classificador estava funcionando bem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECNf3rQKxv1A"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcc8fkjExyDk",
        "outputId": "a79e9521-f0ca-4936-a751-1e25cd7b85ed"
      },
      "source": [
        "dados = pd.read_csv('/content/drive/MyDrive/BancodeDadosIC - Joelho/banco_canal4')\r\n",
        "dados.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(86, 1502)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "sug_0t2TyNUb",
        "outputId": "fa4e6661-e318-4115-f87f-061cabf72b25"
      },
      "source": [
        "dados.drop(columns=['Unnamed: 0'], inplace=True)\r\n",
        "dados.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1462</th>\n",
              "      <th>1463</th>\n",
              "      <th>1464</th>\n",
              "      <th>1465</th>\n",
              "      <th>1466</th>\n",
              "      <th>1467</th>\n",
              "      <th>1468</th>\n",
              "      <th>1469</th>\n",
              "      <th>1470</th>\n",
              "      <th>1471</th>\n",
              "      <th>1472</th>\n",
              "      <th>1473</th>\n",
              "      <th>1474</th>\n",
              "      <th>1475</th>\n",
              "      <th>1476</th>\n",
              "      <th>1477</th>\n",
              "      <th>1478</th>\n",
              "      <th>1479</th>\n",
              "      <th>1480</th>\n",
              "      <th>1481</th>\n",
              "      <th>1482</th>\n",
              "      <th>1483</th>\n",
              "      <th>1484</th>\n",
              "      <th>1485</th>\n",
              "      <th>1486</th>\n",
              "      <th>1487</th>\n",
              "      <th>1488</th>\n",
              "      <th>1489</th>\n",
              "      <th>1490</th>\n",
              "      <th>1491</th>\n",
              "      <th>1492</th>\n",
              "      <th>1493</th>\n",
              "      <th>1494</th>\n",
              "      <th>1495</th>\n",
              "      <th>1496</th>\n",
              "      <th>1497</th>\n",
              "      <th>1498</th>\n",
              "      <th>1499</th>\n",
              "      <th>1500</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0248</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>-0.0263</td>\n",
              "      <td>-0.0608</td>\n",
              "      <td>-0.0713</td>\n",
              "      <td>-0.0346</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0135</td>\n",
              "      <td>-0.0173</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0300</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0353</td>\n",
              "      <td>-0.0480</td>\n",
              "      <td>-0.0540</td>\n",
              "      <td>-0.0473</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0787</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>0.0330</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 1501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        1       2       3       4  ...    1498    1499    1500  class\n",
              "0 -0.0046 -0.0023 -0.0038 -0.0075  ...  0.0172  0.0165  0.0150      0\n",
              "1  0.0120  0.0082  0.0067  0.0045  ... -0.0060 -0.0053 -0.0083      0\n",
              "2 -0.0083 -0.0075 -0.0075 -0.0060  ...  0.0060  0.0067  0.0045      0\n",
              "3 -0.0015 -0.0008 -0.0015 -0.0023  ...  0.0105  0.0090  0.0045      0\n",
              "4 -0.0008 -0.0015 -0.0046 -0.0060  ... -0.0008 -0.0038 -0.0023      0\n",
              "\n",
              "[5 rows x 1501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "1_I9EVVNyxEq",
        "outputId": "3f3ff729-4972-4b4e-d1a0-89dabc895ff9"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "y = dados['class']\r\n",
        "dados.drop(columns=['class'], inplace=True)\r\n",
        "x = dados\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=1)\r\n",
        "X_train.head()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1461</th>\n",
              "      <th>1462</th>\n",
              "      <th>1463</th>\n",
              "      <th>1464</th>\n",
              "      <th>1465</th>\n",
              "      <th>1466</th>\n",
              "      <th>1467</th>\n",
              "      <th>1468</th>\n",
              "      <th>1469</th>\n",
              "      <th>1470</th>\n",
              "      <th>1471</th>\n",
              "      <th>1472</th>\n",
              "      <th>1473</th>\n",
              "      <th>1474</th>\n",
              "      <th>1475</th>\n",
              "      <th>1476</th>\n",
              "      <th>1477</th>\n",
              "      <th>1478</th>\n",
              "      <th>1479</th>\n",
              "      <th>1480</th>\n",
              "      <th>1481</th>\n",
              "      <th>1482</th>\n",
              "      <th>1483</th>\n",
              "      <th>1484</th>\n",
              "      <th>1485</th>\n",
              "      <th>1486</th>\n",
              "      <th>1487</th>\n",
              "      <th>1488</th>\n",
              "      <th>1489</th>\n",
              "      <th>1490</th>\n",
              "      <th>1491</th>\n",
              "      <th>1492</th>\n",
              "      <th>1493</th>\n",
              "      <th>1494</th>\n",
              "      <th>1495</th>\n",
              "      <th>1496</th>\n",
              "      <th>1497</th>\n",
              "      <th>1498</th>\n",
              "      <th>1499</th>\n",
              "      <th>1500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>0.0862</td>\n",
              "      <td>-0.0353</td>\n",
              "      <td>-0.2385</td>\n",
              "      <td>-0.4110</td>\n",
              "      <td>-0.1673</td>\n",
              "      <td>-0.0825</td>\n",
              "      <td>-0.1073</td>\n",
              "      <td>0.0540</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>0.0555</td>\n",
              "      <td>0.0495</td>\n",
              "      <td>-0.0135</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0645</td>\n",
              "      <td>0.1747</td>\n",
              "      <td>0.1785</td>\n",
              "      <td>0.2670</td>\n",
              "      <td>0.2475</td>\n",
              "      <td>-0.0533</td>\n",
              "      <td>-0.4575</td>\n",
              "      <td>-0.4530</td>\n",
              "      <td>-0.1260</td>\n",
              "      <td>-0.1373</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>-0.0458</td>\n",
              "      <td>0.1102</td>\n",
              "      <td>0.1177</td>\n",
              "      <td>0.0690</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>-0.0390</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0787</td>\n",
              "      <td>0.0600</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0517</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0412</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0443</td>\n",
              "      <td>-0.0578</td>\n",
              "      <td>-0.0638</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.1252</td>\n",
              "      <td>0.2047</td>\n",
              "      <td>0.1987</td>\n",
              "      <td>0.0892</td>\n",
              "      <td>-0.1170</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>0.0802</td>\n",
              "      <td>-0.1358</td>\n",
              "      <td>-0.1223</td>\n",
              "      <td>-0.0653</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0510</td>\n",
              "      <td>-0.0488</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0900</td>\n",
              "      <td>0.1080</td>\n",
              "      <td>0.0937</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.2123</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0263</td>\n",
              "      <td>-0.2625</td>\n",
              "      <td>-0.2895</td>\n",
              "      <td>-0.0750</td>\n",
              "      <td>0.0652</td>\n",
              "      <td>0.1267</td>\n",
              "      <td>0.2685</td>\n",
              "      <td>0.1717</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0427</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.1312</td>\n",
              "      <td>0.1680</td>\n",
              "      <td>0.1065</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>-0.0188</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0413</td>\n",
              "      <td>-0.0256</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0263</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0285</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0300</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>-0.0383</td>\n",
              "      <td>-0.0375</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0255</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0322</td>\n",
              "      <td>0.0712</td>\n",
              "      <td>0.1020</td>\n",
              "      <td>0.1327</td>\n",
              "      <td>0.1192</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0495</td>\n",
              "      <td>-0.0346</td>\n",
              "      <td>-0.0420</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0248</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0173</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0420</td>\n",
              "      <td>-0.0473</td>\n",
              "      <td>-0.0473</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0480</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>0.0847</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0383</td>\n",
              "      <td>-0.0675</td>\n",
              "      <td>-0.0600</td>\n",
              "      <td>-0.0390</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0652</td>\n",
              "      <td>0.0765</td>\n",
              "      <td>0.0412</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0353</td>\n",
              "      <td>-0.0436</td>\n",
              "      <td>-0.0540</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0210</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>-0.0135</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0420</td>\n",
              "      <td>0.0607</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0068</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 1500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1       2       3       4  ...    1497    1498    1499    1500\n",
              "62  0.0105  0.0735  0.0862 -0.0353  ...  0.0652  0.1267  0.2685  0.1717\n",
              "60  0.0090  0.0082  0.0067  0.0067  ...  0.0030 -0.0008 -0.0023 -0.0023\n",
              "49  0.0007  0.0015  0.0030  0.0030  ... -0.0008 -0.0008  0.0015  0.0015\n",
              "33  0.1312  0.1680  0.1065  0.0082  ... -0.0008  0.0480  0.0825  0.0847\n",
              "39  0.0052 -0.0143 -0.0270 -0.0181  ...  0.0030  0.0000 -0.0038 -0.0068\n",
              "\n",
              "[5 rows x 1500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPy0e48e0SAY",
        "outputId": "e5716a6e-445e-4bf4-8714-10344ed4ab66"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "\r\n",
        "scaler.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 4.45542279e-01,  2.05481292e+00,  2.75106658e+00, ...,\n",
              "         4.07151213e+00,  6.25190611e+00,  5.68082736e+00],\n",
              "       [ 4.07766673e-01,  3.26032611e-01,  2.60019372e-01, ...,\n",
              "        -5.90152474e-02, -1.90439348e-01, -7.12954688e-02],\n",
              "       [ 1.98741657e-01,  1.48653927e-01,  1.44083842e-01, ...,\n",
              "        -5.90152474e-02, -1.00037159e-01,  5.43256045e-02],\n",
              "       ...,\n",
              "       [-2.50095494e+00, -2.01695449e+00,  1.08410166e+00, ...,\n",
              "         1.12685106e-01,  2.36710994e-02,  2.75815392e-01],\n",
              "       [ 3.12068473e-01,  1.48653927e-01,  9.70829509e-02, ...,\n",
              "         8.67680719e-02, -2.86670099e-02,  1.03912870e-01],\n",
              "       [ 1.43337436e-01,  4.80510913e-02,  3.08116950e-03, ...,\n",
              "        -3.01987446e-01, -3.85517755e-01, -3.42372522e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K02mMF808x9"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "rede = MLPClassifier(activation='identity',random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXTcoYPz8M9n",
        "outputId": "d99077b7-0a4c-4dd4-94eb-986827c4f6b0"
      },
      "source": [
        "layers = [(10,),(20,),(30,),(40,),(50,),(2,2),(3,3),(4,4),(5,5)]\r\n",
        "df = pd.DataFrame(columns=['Camadas', 'Acur√°cia', 'Precis√£o', 'Recall', 'F1 Score'])\r\n",
        "i = 0\r\n",
        "for layer in layers:\r\n",
        "  lista = []\r\n",
        "  rede = MLPClassifier(hidden_layer_sizes=layer, random_state=1);\r\n",
        "  rede.fit(X_train,Y_train);\r\n",
        "  pred = rede.predict(X_test);\r\n",
        "  acc = accuracy_score(Y_test, pred);\r\n",
        "  prec = precision_score(Y_test, pred);\r\n",
        "  rec = recall_score(Y_test, pred);\r\n",
        "  f1 = f1_score(Y_test, pred);\r\n",
        "  lista.append(layer)\r\n",
        "  lista.append(acc)\r\n",
        "  lista.append(prec)\r\n",
        "  lista.append(rec)\r\n",
        "  lista.append(f1)\r\n",
        "  df.loc[i] = lista\r\n",
        "  i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3GWUbw-TpS",
        "outputId": "974b2494-d8fa-4f3c-be3b-87e65367b3b2"
      },
      "source": [
        "ativa√ß√µes = ['relu', 'identity', 'logistic', 'tanh']\r\n",
        "layers = [(10,),(20,),(30,),(40,),(50,),(2,2),(3,3),(4,4),(5,5)]\r\n",
        "df = pd.DataFrame(columns=['Ativa√ß√£o','Camadas', 'Acur√°cia', 'Precis√£o', 'Recall', 'F1 Score'])\r\n",
        "i = 0\r\n",
        "for ativa√ß√£o in ativa√ß√µes:\r\n",
        "  i = i\r\n",
        "  for layer in layers:\r\n",
        "    lista = []\r\n",
        "    rede = MLPClassifier(activation=ativa√ß√£o,hidden_layer_sizes=layer,max_iter=300 ,random_state=1);\r\n",
        "    rede.fit(X_train,Y_train);\r\n",
        "    pred = rede.predict(X_test);\r\n",
        "    acc = accuracy_score(Y_test, pred);\r\n",
        "    prec = precision_score(Y_test, pred);\r\n",
        "    rec = recall_score(Y_test, pred);\r\n",
        "    f1 = f1_score(Y_test, pred);\r\n",
        "    lista.append(ativa√ß√£o)\r\n",
        "    lista.append(layer)\r\n",
        "    lista.append(acc)\r\n",
        "    lista.append(prec)\r\n",
        "    lista.append(rec)\r\n",
        "    lista.append(f1)\r\n",
        "    df.loc[i] = lista\r\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhlvpbodQd8"
      },
      "source": [
        "\r\n",
        "variar as camadas escondidas\r\n",
        "\r\n",
        "aumentando e diminuindo os neuronios\r\n",
        "\r\n",
        "testar tbm as diferentes ativa√ß√µes\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJZi56O_Zm-"
      },
      "source": [
        "df_tanh = df.loc[df['Ativa√ß√£o']=='tanh']\r\n",
        "df_relu = df.loc[df['Ativa√ß√£o']=='relu']\r\n",
        "df_identity = df.loc[df['Ativa√ß√£o']=='identity']\r\n",
        "df_logistic = df.loc[df['Ativa√ß√£o']=='logistic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "V1JSxSsq7_KU",
        "outputId": "dc0ab494-abeb-4e84-a009-b9242f8ae40e"
      },
      "source": [
        "df_identity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ativa√ß√£o</th>\n",
              "      <th>Camadas</th>\n",
              "      <th>Acur√°cia</th>\n",
              "      <th>Precis√£o</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>identity</td>\n",
              "      <td>(10,)</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.642857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>identity</td>\n",
              "      <td>(20,)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>identity</td>\n",
              "      <td>(30,)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>identity</td>\n",
              "      <td>(40,)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>identity</td>\n",
              "      <td>(50,)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.705882</td>\n",
              "      <td>0.800000</td>\n",
              "      <td>0.750000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>identity</td>\n",
              "      <td>(2, 2)</td>\n",
              "      <td>0.653846</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.689655</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>identity</td>\n",
              "      <td>(3, 3)</td>\n",
              "      <td>0.730769</td>\n",
              "      <td>0.722222</td>\n",
              "      <td>0.866667</td>\n",
              "      <td>0.787879</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>identity</td>\n",
              "      <td>(4, 4)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "      <td>0.733333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>identity</td>\n",
              "      <td>(5, 5)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Ativa√ß√£o Camadas  Acur√°cia  Precis√£o    Recall  F1 Score\n",
              "9   identity   (10,)  0.615385  0.692308  0.600000  0.642857\n",
              "10  identity   (20,)  0.692308  0.733333  0.733333  0.733333\n",
              "11  identity   (30,)  0.692308  0.733333  0.733333  0.733333\n",
              "12  identity   (40,)  0.692308  0.705882  0.800000  0.750000\n",
              "13  identity   (50,)  0.692308  0.705882  0.800000  0.750000\n",
              "14  identity  (2, 2)  0.653846  0.714286  0.666667  0.689655\n",
              "15  identity  (3, 3)  0.730769  0.722222  0.866667  0.787879\n",
              "16  identity  (4, 4)  0.692308  0.733333  0.733333  0.733333\n",
              "17  identity  (5, 5)  0.692308  0.818182  0.600000  0.692308"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wo7aLwM7_Nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bd0fe0d2-72f6-472e-e561-196bc0abc558"
      },
      "source": [
        "melhor_rede = MLPClassifier(hidden_layer_sizes=(4,4), max_iter=270 ,activation='relu', random_state=1)\r\n",
        "melhor_rede.fit(X_train, Y_train)\r\n",
        "pred_melhor_rede = melhor_rede.predict(X_test)\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (270) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrcIpCteHyki",
        "outputId": "183eca05-20f8-4f34-c445-10a4a30e7de3"
      },
      "source": [
        "confusion_matrix(Y_test, pred_melhor_rede)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8,  3],\n",
              "       [ 4, 11]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    }
  ]
}