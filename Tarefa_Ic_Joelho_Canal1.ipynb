{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarefa_Ic_Joelho_Canal1.ipynb",
      "provenance": [],
      "mount_file_id": "1RJHojoJNMitHjqUFJA3wfrDKgGgyTshB",
      "authorship_tag": "ABX9TyOQ36z/k0pgNzmZQ+Jwcody",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroOlavo/Bolsa_IC/blob/main/Tarefa_Ic_Joelho_Canal1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjEjKqHIQ6td"
      },
      "source": [
        "Teste de rede neural com o banco que criei a partir do canal 1, foram usadas 4 métricas para avaliar se o classificador estava funcionando bem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECNf3rQKxv1A"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcc8fkjExyDk",
        "outputId": "7cba9ce2-eb04-4c3b-d621-ac00d6f2bcb5"
      },
      "source": [
        "dados = pd.read_csv('/content/drive/MyDrive/BancodeDadosIC - Joelho/banco_canal1')\r\n",
        "dados.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(86, 1502)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "sug_0t2TyNUb",
        "outputId": "851e0ab2-3dd1-45d6-88db-e2b18be337be"
      },
      "source": [
        "dados.drop(columns=['Unnamed: 0'], inplace=True)\r\n",
        "dados.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1462</th>\n",
              "      <th>1463</th>\n",
              "      <th>1464</th>\n",
              "      <th>1465</th>\n",
              "      <th>1466</th>\n",
              "      <th>1467</th>\n",
              "      <th>1468</th>\n",
              "      <th>1469</th>\n",
              "      <th>1470</th>\n",
              "      <th>1471</th>\n",
              "      <th>1472</th>\n",
              "      <th>1473</th>\n",
              "      <th>1474</th>\n",
              "      <th>1475</th>\n",
              "      <th>1476</th>\n",
              "      <th>1477</th>\n",
              "      <th>1478</th>\n",
              "      <th>1479</th>\n",
              "      <th>1480</th>\n",
              "      <th>1481</th>\n",
              "      <th>1482</th>\n",
              "      <th>1483</th>\n",
              "      <th>1484</th>\n",
              "      <th>1485</th>\n",
              "      <th>1486</th>\n",
              "      <th>1487</th>\n",
              "      <th>1488</th>\n",
              "      <th>1489</th>\n",
              "      <th>1490</th>\n",
              "      <th>1491</th>\n",
              "      <th>1492</th>\n",
              "      <th>1493</th>\n",
              "      <th>1494</th>\n",
              "      <th>1495</th>\n",
              "      <th>1496</th>\n",
              "      <th>1497</th>\n",
              "      <th>1498</th>\n",
              "      <th>1499</th>\n",
              "      <th>1500</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0517</td>\n",
              "      <td>0.0615</td>\n",
              "      <td>0.0615</td>\n",
              "      <td>0.0532</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0135</td>\n",
              "      <td>-0.0285</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0188</td>\n",
              "      <td>-0.0233</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.0817</td>\n",
              "      <td>0.0832</td>\n",
              "      <td>0.0555</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0383</td>\n",
              "      <td>-0.0458</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0368</td>\n",
              "      <td>-0.0368</td>\n",
              "      <td>-0.0458</td>\n",
              "      <td>-0.0698</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.0773</td>\n",
              "      <td>-0.0615</td>\n",
              "      <td>-0.0346</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0502</td>\n",
              "      <td>0.0697</td>\n",
              "      <td>0.0735</td>\n",
              "      <td>0.0885</td>\n",
              "      <td>0.0975</td>\n",
              "      <td>0.1005</td>\n",
              "      <td>0.0750</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>-0.0503</td>\n",
              "      <td>-0.0630</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0173</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0256</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0225</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0188</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0353</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>-0.0188</td>\n",
              "      <td>-0.0368</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0240</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.0390</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0307</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0285</td>\n",
              "      <td>-0.0278</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0233</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>0.0330</td>\n",
              "      <td>0.0322</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0225</td>\n",
              "      <td>-0.0405</td>\n",
              "      <td>-0.0405</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0256</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.0262</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0352</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0330</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        1       2       3       4  ...    1498    1499    1500  class\n",
              "0 -0.0008 -0.0008 -0.0008  0.0007  ... -0.0368 -0.0458 -0.0698      0\n",
              "1 -0.0773 -0.0615 -0.0346 -0.0015  ...  0.0030  0.0045  0.0030      0\n",
              "2  0.0060  0.0067  0.0090  0.0082  ... -0.0015  0.0007 -0.0008      0\n",
              "3 -0.0015  0.0015  0.0007  0.0015  ... -0.0270 -0.0285 -0.0278      0\n",
              "4 -0.0293 -0.0203 -0.0015  0.0135  ...  0.0330  0.0315  0.0195      0\n",
              "\n",
              "[5 rows x 1501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "1_I9EVVNyxEq",
        "outputId": "c0f54d70-562b-4647-a6af-e300b8e44bf1"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "y = dados['class']\r\n",
        "dados.drop(columns=['class'], inplace=True)\r\n",
        "x = dados\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=1)\r\n",
        "X_train.head()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1461</th>\n",
              "      <th>1462</th>\n",
              "      <th>1463</th>\n",
              "      <th>1464</th>\n",
              "      <th>1465</th>\n",
              "      <th>1466</th>\n",
              "      <th>1467</th>\n",
              "      <th>1468</th>\n",
              "      <th>1469</th>\n",
              "      <th>1470</th>\n",
              "      <th>1471</th>\n",
              "      <th>1472</th>\n",
              "      <th>1473</th>\n",
              "      <th>1474</th>\n",
              "      <th>1475</th>\n",
              "      <th>1476</th>\n",
              "      <th>1477</th>\n",
              "      <th>1478</th>\n",
              "      <th>1479</th>\n",
              "      <th>1480</th>\n",
              "      <th>1481</th>\n",
              "      <th>1482</th>\n",
              "      <th>1483</th>\n",
              "      <th>1484</th>\n",
              "      <th>1485</th>\n",
              "      <th>1486</th>\n",
              "      <th>1487</th>\n",
              "      <th>1488</th>\n",
              "      <th>1489</th>\n",
              "      <th>1490</th>\n",
              "      <th>1491</th>\n",
              "      <th>1492</th>\n",
              "      <th>1493</th>\n",
              "      <th>1494</th>\n",
              "      <th>1495</th>\n",
              "      <th>1496</th>\n",
              "      <th>1497</th>\n",
              "      <th>1498</th>\n",
              "      <th>1499</th>\n",
              "      <th>1500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0030</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0233</td>\n",
              "      <td>-0.0263</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0323</td>\n",
              "      <td>-0.0353</td>\n",
              "      <td>-0.0398</td>\n",
              "      <td>-0.0390</td>\n",
              "      <td>-0.0390</td>\n",
              "      <td>-0.0390</td>\n",
              "      <td>-0.0390</td>\n",
              "      <td>-0.0383</td>\n",
              "      <td>-0.0368</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0248</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0165</td>\n",
              "      <td>0.0285</td>\n",
              "      <td>0.0307</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.0420</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0428</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0300</td>\n",
              "      <td>-0.0495</td>\n",
              "      <td>-0.0608</td>\n",
              "      <td>-0.0593</td>\n",
              "      <td>-0.0458</td>\n",
              "      <td>-0.0420</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0405</td>\n",
              "      <td>0.0382</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.0360</td>\n",
              "      <td>0.0442</td>\n",
              "      <td>0.0547</td>\n",
              "      <td>0.0570</td>\n",
              "      <td>0.0502</td>\n",
              "      <td>0.0270</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0563</td>\n",
              "      <td>-0.0570</td>\n",
              "      <td>-0.0375</td>\n",
              "      <td>-0.0248</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0773</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0202</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0691</td>\n",
              "      <td>-0.0645</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0683</td>\n",
              "      <td>-0.0540</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0532</td>\n",
              "      <td>0.0727</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0533</td>\n",
              "      <td>-0.0833</td>\n",
              "      <td>-0.0645</td>\n",
              "      <td>-0.0338</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0173</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>0.0720</td>\n",
              "      <td>0.0862</td>\n",
              "      <td>0.0840</td>\n",
              "      <td>0.0787</td>\n",
              "      <td>0.0465</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0210</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0270</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1       2       3       4  ...    1497    1498    1499    1500\n",
              "62 -0.0046 -0.0030 -0.0008  0.0015  ... -0.0030 -0.0038 -0.0038 -0.0030\n",
              "60 -0.0008 -0.0030 -0.0068 -0.0068  ... -0.0060 -0.0068 -0.0098 -0.0105\n",
              "49  0.0052  0.0045  0.0052  0.0082  ...  0.0015  0.0045  0.0037  0.0067\n",
              "33  0.0015  0.0007  0.0007  0.0022  ... -0.0008  0.0007  0.0030  0.0045\n",
              "39 -0.0008  0.0015  0.0052  0.0090  ... -0.0210  0.0277  0.0292  0.0270\n",
              "\n",
              "[5 rows x 1500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPy0e48e0SAY",
        "outputId": "92089e58-aac1-4688-f301-5a4ef38c8bf4"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "\r\n",
        "scaler.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 8.04655033e-02,  9.57367913e-02,  9.84828051e-02, ...,\n",
              "         1.01834258e-01,  8.93192860e-02,  1.10278999e-01],\n",
              "       [ 1.39098085e-01,  9.57367913e-02,  1.40227433e-03, ...,\n",
              "         5.85312423e-02,  2.60153260e-03,  2.45009996e-05],\n",
              "       [ 2.31675845e-01,  2.11485196e-01,  1.95563336e-01, ...,\n",
              "         2.21639266e-01,  1.97716478e-01,  2.52874817e-01],\n",
              "       ...,\n",
              "       [ 1.08802013e+00,  1.17142530e+00,  1.53042063e+00, ...,\n",
              "        -1.24055921e+00, -1.10304982e+00, -9.92265982e-01],\n",
              "       [ 2.20875106e-01,  1.65185834e-01,  1.11426876e-01, ...,\n",
              "         1.66788780e-01,  1.76037039e-01,  1.32329899e-01],\n",
              "       [ 1.39098085e-01,  1.52839337e-01,  9.84828051e-02, ...,\n",
              "         2.31743303e-01,  2.41075354e-01,  2.42584397e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K02mMF808x9"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "rede = MLPClassifier(activation='identity',random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXTcoYPz8M9n",
        "outputId": "d4a4a156-c29e-4f0d-ec23-3268c311b84e"
      },
      "source": [
        "layers = [(10,),(20,),(30,),(40,),(50,),(2,2),(3,3),(4,4),(5,5)]\r\n",
        "df = pd.DataFrame(columns=['Camadas', 'Acurácia', 'Precisão', 'Recall', 'F1 Score'])\r\n",
        "i = 0\r\n",
        "for layer in layers:\r\n",
        "  lista = []\r\n",
        "  rede = MLPClassifier(hidden_layer_sizes=layer, random_state=1);\r\n",
        "  rede.fit(X_train,Y_train);\r\n",
        "  pred = rede.predict(X_test);\r\n",
        "  acc = accuracy_score(Y_test, pred);\r\n",
        "  prec = precision_score(Y_test, pred);\r\n",
        "  rec = recall_score(Y_test, pred);\r\n",
        "  f1 = f1_score(Y_test, pred);\r\n",
        "  lista.append(layer)\r\n",
        "  lista.append(acc)\r\n",
        "  lista.append(prec)\r\n",
        "  lista.append(rec)\r\n",
        "  lista.append(f1)\r\n",
        "  df.loc[i] = lista\r\n",
        "  i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3GWUbw-TpS",
        "outputId": "f6afb2c6-98d3-4e0e-f8d1-72bb65fb2947"
      },
      "source": [
        "ativações = ['relu', 'identity', 'logistic', 'tanh']\r\n",
        "layers = [(10,),(20,),(30,),(40,),(50,),(2,2),(3,3),(4,4),(5,5)]\r\n",
        "df = pd.DataFrame(columns=['Ativação','Camadas', 'Acurácia', 'Precisão', 'Recall', 'F1 Score'])\r\n",
        "i = 0\r\n",
        "for ativação in ativações:\r\n",
        "  i = i\r\n",
        "  for layer in layers:\r\n",
        "    lista = []\r\n",
        "    rede = MLPClassifier(activation=ativação,hidden_layer_sizes=layer,max_iter=300 ,random_state=1);\r\n",
        "    rede.fit(X_train,Y_train);\r\n",
        "    pred = rede.predict(X_test);\r\n",
        "    acc = accuracy_score(Y_test, pred);\r\n",
        "    prec = precision_score(Y_test, pred);\r\n",
        "    rec = recall_score(Y_test, pred);\r\n",
        "    f1 = f1_score(Y_test, pred);\r\n",
        "    lista.append(ativação)\r\n",
        "    lista.append(layer)\r\n",
        "    lista.append(acc)\r\n",
        "    lista.append(prec)\r\n",
        "    lista.append(rec)\r\n",
        "    lista.append(f1)\r\n",
        "    df.loc[i] = lista\r\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhlvpbodQd8"
      },
      "source": [
        "\r\n",
        "variar as camadas escondidas\r\n",
        "\r\n",
        "aumentando e diminuindo os neuronios\r\n",
        "\r\n",
        "testar tbm as diferentes ativações\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJZi56O_Zm-"
      },
      "source": [
        "df_tanh = df.loc[df['Ativação']=='tanh']\r\n",
        "df_relu = df.loc[df['Ativação']=='relu']\r\n",
        "df_identity = df.loc[df['Ativação']=='identity']\r\n",
        "df_logistic = df.loc[df['Ativação']=='logistic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHcBj_10LZbG",
        "outputId": "2e7d8a7c-111e-4053-edca-7d1c8875665e"
      },
      "source": [
        "df.Acurácia.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6153846153846154"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "V1JSxSsq7_KU",
        "outputId": "3ec19a0b-0c93-4d47-d9a7-d51e3968af01"
      },
      "source": [
        "df_logistic"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ativação</th>\n",
              "      <th>Camadas</th>\n",
              "      <th>Acurácia</th>\n",
              "      <th>Precisão</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(10,)</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(20,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(30,)</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.592593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(40,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(50,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.615385</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.571429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(2, 2)</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.731707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(3, 3)</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.133333</td>\n",
              "      <td>0.235294</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(4, 4)</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.266667</td>\n",
              "      <td>0.380952</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>logistic</td>\n",
              "      <td>(5, 5)</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.700000</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.560000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    Ativação Camadas  Acurácia  Precisão    Recall  F1 Score\n",
              "18  logistic   (10,)  0.500000  0.600000  0.400000  0.480000\n",
              "19  logistic   (20,)  0.538462  0.636364  0.466667  0.538462\n",
              "20  logistic   (30,)  0.576923  0.666667  0.533333  0.592593\n",
              "21  logistic   (40,)  0.538462  0.615385  0.533333  0.571429\n",
              "22  logistic   (50,)  0.538462  0.615385  0.533333  0.571429\n",
              "23  logistic  (2, 2)  0.576923  0.576923  1.000000  0.731707\n",
              "24  logistic  (3, 3)  0.500000  1.000000  0.133333  0.235294\n",
              "25  logistic  (4, 4)  0.500000  0.666667  0.266667  0.380952\n",
              "26  logistic  (5, 5)  0.576923  0.700000  0.466667  0.560000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wo7aLwM7_Nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07229173-fab5-4720-d11b-27a620abb009"
      },
      "source": [
        "melhor_rede = MLPClassifier(hidden_layer_sizes=(3,3) ,activation='logistic', max_iter=300, random_state=1)\r\n",
        "melhor_rede.fit(X_train, Y_train)\r\n",
        "pred_melhor_rede = melhor_rede.predict(X_test)\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrcIpCteHyki",
        "outputId": "1660b66c-aeab-4c93-928a-66e7ab857292"
      },
      "source": [
        "confusion_matrix(Y_test, pred_melhor_rede)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[11,  0],\n",
              "       [13,  2]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5qyD2WPKFrL",
        "outputId": "bedb0a5b-b77b-412e-c008-face0e84b38b"
      },
      "source": [
        "precision_score(Y_test, pred_melhor_rede)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ej9ruhKpbC",
        "outputId": "2f4edc02-06cb-4694-cda3-4a3950cc54e2"
      },
      "source": [
        "Y_test.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15\n",
              "0    11\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR-DXrdhAcGw",
        "outputId": "f4921b66-915b-4d60-b2c5-77d5fdad8f9e"
      },
      "source": [
        "df.groupby(by=['Ativação', 'Camadas']).Precisão.max().sort_values(ascending=False)[:5]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ativação  Camadas\n",
              "logistic  (3, 3)     1.000000\n",
              "          (5, 5)     0.700000\n",
              "relu      (5, 5)     0.700000\n",
              "logistic  (30,)      0.666667\n",
              "          (4, 4)     0.666667\n",
              "Name: Precisão, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXcFXPlxDOJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "413dbb5d-1535-441b-d058-121c2fa88811"
      },
      "source": [
        "df.groupby(by=['Ativação', 'Camadas']).Acurácia.max().sort_values(ascending=False)[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ativação  Camadas\n",
              "identity  (3, 3)     0.615385\n",
              "tanh      (3, 3)     0.615385\n",
              "          (50,)      0.576923\n",
              "relu      (20,)      0.576923\n",
              "logistic  (30,)      0.576923\n",
              "Name: Acurácia, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "kLCpgoOePHFW",
        "outputId": "ab2da198-ac50-4cd5-9c9a-e290e7ee644b"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve\r\n",
        "plot_roc_curve(melhor_rede, X_test, Y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7ff27596ed30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpIpQ8BRgja"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}