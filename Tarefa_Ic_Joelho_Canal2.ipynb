{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Tarefa_Ic_Joelho_Canal2.ipynb",
      "provenance": [],
      "mount_file_id": "1FpLHl6mJdg34IXfSDzJ6TKUIr_WvlG0p",
      "authorship_tag": "ABX9TyPPb1CqL9FynGvohP/enEzQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PedroOlavo/Bolsa_IC/blob/main/Tarefa_Ic_Joelho_Canal2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQZhJRgVRVpv"
      },
      "source": [
        "Teste de rede neural com o banco que criei a partir do canal 2, foram usadas 4 m√©tricas para avaliar se o classificador estava funcionando bem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECNf3rQKxv1A"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jcc8fkjExyDk",
        "outputId": "2b17bfb4-df0e-4e82-ee6b-dfa2146eb2e0"
      },
      "source": [
        "dados = pd.read_csv('/content/drive/MyDrive/BancodeDadosIC - Joelho/banco_canal2')\r\n",
        "dados.shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(86, 1502)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "sug_0t2TyNUb",
        "outputId": "90b54482-d074-4cf2-d2d0-cede6c7e0ffb"
      },
      "source": [
        "dados.drop(columns=['Unnamed: 0'], inplace=True)\r\n",
        "dados.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1462</th>\n",
              "      <th>1463</th>\n",
              "      <th>1464</th>\n",
              "      <th>1465</th>\n",
              "      <th>1466</th>\n",
              "      <th>1467</th>\n",
              "      <th>1468</th>\n",
              "      <th>1469</th>\n",
              "      <th>1470</th>\n",
              "      <th>1471</th>\n",
              "      <th>1472</th>\n",
              "      <th>1473</th>\n",
              "      <th>1474</th>\n",
              "      <th>1475</th>\n",
              "      <th>1476</th>\n",
              "      <th>1477</th>\n",
              "      <th>1478</th>\n",
              "      <th>1479</th>\n",
              "      <th>1480</th>\n",
              "      <th>1481</th>\n",
              "      <th>1482</th>\n",
              "      <th>1483</th>\n",
              "      <th>1484</th>\n",
              "      <th>1485</th>\n",
              "      <th>1486</th>\n",
              "      <th>1487</th>\n",
              "      <th>1488</th>\n",
              "      <th>1489</th>\n",
              "      <th>1490</th>\n",
              "      <th>1491</th>\n",
              "      <th>1492</th>\n",
              "      <th>1493</th>\n",
              "      <th>1494</th>\n",
              "      <th>1495</th>\n",
              "      <th>1496</th>\n",
              "      <th>1497</th>\n",
              "      <th>1498</th>\n",
              "      <th>1499</th>\n",
              "      <th>1500</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0217</td>\n",
              "      <td>0.0345</td>\n",
              "      <td>0.0382</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 1501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        1       2       3       4  ...    1498    1499    1500  class\n",
              "0 -0.0038 -0.0068 -0.0053  0.0015  ...  0.0022 -0.0008 -0.0015      0\n",
              "1 -0.0030 -0.0030  0.0052  0.0067  ...  0.0007  0.0007  0.0015      0\n",
              "2  0.0045  0.0045  0.0052  0.0037  ...  0.0030  0.0037  0.0045      0\n",
              "3 -0.0030 -0.0030 -0.0015  0.0000  ... -0.0015 -0.0038 -0.0008      0\n",
              "4 -0.0030 -0.0053 -0.0046 -0.0083  ... -0.0083 -0.0091 -0.0046      0\n",
              "\n",
              "[5 rows x 1501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 244
        },
        "id": "1_I9EVVNyxEq",
        "outputId": "43172efc-bea3-4ff7-98cf-77299b4e9e0a"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\r\n",
        "y = dados['class']\r\n",
        "dados.drop(columns=['class'], inplace=True)\r\n",
        "x = dados\r\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(x, y, test_size=0.3, random_state=1)\r\n",
        "X_train.head()\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>...</th>\n",
              "      <th>1461</th>\n",
              "      <th>1462</th>\n",
              "      <th>1463</th>\n",
              "      <th>1464</th>\n",
              "      <th>1465</th>\n",
              "      <th>1466</th>\n",
              "      <th>1467</th>\n",
              "      <th>1468</th>\n",
              "      <th>1469</th>\n",
              "      <th>1470</th>\n",
              "      <th>1471</th>\n",
              "      <th>1472</th>\n",
              "      <th>1473</th>\n",
              "      <th>1474</th>\n",
              "      <th>1475</th>\n",
              "      <th>1476</th>\n",
              "      <th>1477</th>\n",
              "      <th>1478</th>\n",
              "      <th>1479</th>\n",
              "      <th>1480</th>\n",
              "      <th>1481</th>\n",
              "      <th>1482</th>\n",
              "      <th>1483</th>\n",
              "      <th>1484</th>\n",
              "      <th>1485</th>\n",
              "      <th>1486</th>\n",
              "      <th>1487</th>\n",
              "      <th>1488</th>\n",
              "      <th>1489</th>\n",
              "      <th>1490</th>\n",
              "      <th>1491</th>\n",
              "      <th>1492</th>\n",
              "      <th>1493</th>\n",
              "      <th>1494</th>\n",
              "      <th>1495</th>\n",
              "      <th>1496</th>\n",
              "      <th>1497</th>\n",
              "      <th>1498</th>\n",
              "      <th>1499</th>\n",
              "      <th>1500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>62</th>\n",
              "      <td>0.0375</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0818</td>\n",
              "      <td>-0.0998</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0330</td>\n",
              "      <td>0.0420</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0405</td>\n",
              "      <td>-0.0225</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0382</td>\n",
              "      <td>0.0442</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0195</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0277</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.0427</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>0.0090</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.0412</td>\n",
              "      <td>0.0172</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0225</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0105</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60</th>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0067</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>-0.0068</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>49</th>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>-0.0038</td>\n",
              "      <td>-0.0015</td>\n",
              "      <td>0.0000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>33</th>\n",
              "      <td>0.0495</td>\n",
              "      <td>0.0592</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0181</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>-0.0113</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0263</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0188</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>0.0135</td>\n",
              "      <td>0.0525</td>\n",
              "      <td>0.0487</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>0.0052</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0112</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0480</td>\n",
              "      <td>0.0615</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0256</td>\n",
              "      <td>-0.0293</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>0.0150</td>\n",
              "      <td>0.0382</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0248</td>\n",
              "      <td>-0.0285</td>\n",
              "      <td>-0.0188</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0143</td>\n",
              "      <td>-0.0135</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0263</td>\n",
              "      <td>-0.0330</td>\n",
              "      <td>-0.0368</td>\n",
              "      <td>-0.0270</td>\n",
              "      <td>-0.0165</td>\n",
              "      <td>-0.0083</td>\n",
              "      <td>0.0082</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.0787</td>\n",
              "      <td>0.0817</td>\n",
              "      <td>0.0690</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0091</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0046</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>0.0300</td>\n",
              "      <td>0.0315</td>\n",
              "      <td>0.0232</td>\n",
              "      <td>0.0285</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>-0.0075</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>-0.0203</td>\n",
              "      <td>-0.0210</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0240</td>\n",
              "      <td>-0.0308</td>\n",
              "      <td>-0.0195</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0127</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0315</td>\n",
              "      <td>-0.0436</td>\n",
              "      <td>-0.0436</td>\n",
              "      <td>-0.0465</td>\n",
              "      <td>-0.0405</td>\n",
              "      <td>-0.0218</td>\n",
              "      <td>-0.0128</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.0630</td>\n",
              "      <td>0.1237</td>\n",
              "      <td>0.1027</td>\n",
              "      <td>0.0367</td>\n",
              "      <td>0.0382</td>\n",
              "      <td>0.0322</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.0563</td>\n",
              "      <td>-0.0653</td>\n",
              "      <td>-0.0158</td>\n",
              "      <td>0.0180</td>\n",
              "      <td>0.0187</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0037</td>\n",
              "      <td>0.0030</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0023</td>\n",
              "      <td>-0.0060</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0105</td>\n",
              "      <td>0.0157</td>\n",
              "      <td>0.0285</td>\n",
              "      <td>0.0292</td>\n",
              "      <td>0.0210</td>\n",
              "      <td>0.0142</td>\n",
              "      <td>0.0097</td>\n",
              "      <td>0.0045</td>\n",
              "      <td>-0.0098</td>\n",
              "      <td>-0.0135</td>\n",
              "      <td>-0.0030</td>\n",
              "      <td>-0.0008</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>-0.0053</td>\n",
              "      <td>-0.0120</td>\n",
              "      <td>0.0015</td>\n",
              "      <td>0.0120</td>\n",
              "      <td>0.0022</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>0.0060</td>\n",
              "      <td>0.0075</td>\n",
              "      <td>0.0007</td>\n",
              "      <td>-0.0150</td>\n",
              "      <td>-0.0105</td>\n",
              "      <td>-0.0030</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 1500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         1       2       3       4  ...    1497    1498    1499    1500\n",
              "62  0.0375  0.0150 -0.0330 -0.0818  ... -0.0128  0.0052  0.0097  0.0105\n",
              "60 -0.0008  0.0007  0.0015  0.0015  ...  0.0052  0.0052  0.0052  0.0037\n",
              "49 -0.0023 -0.0015  0.0000 -0.0008  ... -0.0015 -0.0038 -0.0015  0.0000\n",
              "33  0.0495  0.0592  0.0315  0.0082  ...  0.0457  0.0787  0.0817  0.0690\n",
              "39  0.0030 -0.0008 -0.0030 -0.0075  ...  0.0007 -0.0150 -0.0105 -0.0030\n",
              "\n",
              "[5 rows x 1500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JPy0e48e0SAY",
        "outputId": "9a2a00d7-b01c-40b7-8b4f-dda7aac275af"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\r\n",
        "scaler = StandardScaler()\r\n",
        "\r\n",
        "scaler.fit_transform(X_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1.87290434e+00,  8.25924642e-01, -1.31714195e+00, ...,\n",
              "         1.73559386e-01,  2.67634622e-01,  4.04169587e-01],\n",
              "       [ 4.95897146e-02,  1.12213953e-01,  1.69247319e-01, ...,\n",
              "         1.73559386e-01,  9.79672167e-02,  1.00316277e-01],\n",
              "       [-2.18194744e-02,  2.41230886e-03,  1.04621699e-01, ...,\n",
              "        -1.85942219e-01, -1.54648698e-01, -6.50156716e-02],\n",
              "       ...,\n",
              "       [ 2.51558704e+00,  2.69754358e+00,  1.84951345e+00, ...,\n",
              "        -2.17897917e-01, -2.71530688e-01, -1.99068603e-01],\n",
              "       [-2.69371330e-01, -3.76902463e-01, -2.87440399e-01, ...,\n",
              "        -3.05776087e-01, -2.71530688e-01, -3.01842517e-01],\n",
              "       [ 1.59083805e-01,  1.52141824e-01,  1.04621699e-01, ...,\n",
              "        -9.40695864e-02, -2.11204500e-01, -1.99068603e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8K02mMF808x9"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "rede = MLPClassifier(activation='identity',random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXTcoYPz8M9n",
        "outputId": "458fa6db-eae9-4d06-e082-8f6b9ee5749f"
      },
      "source": [
        "layers = [(10,),(20,),(30,),(40,),(50,),(2,2),(3,3),(4,4),(5,5)]\r\n",
        "df = pd.DataFrame(columns=['Camadas', 'Acur√°cia', 'Precis√£o', 'Recall', 'F1 Score'])\r\n",
        "i = 0\r\n",
        "for layer in layers:\r\n",
        "  lista = []\r\n",
        "  rede = MLPClassifier(hidden_layer_sizes=layer, random_state=1);\r\n",
        "  rede.fit(X_train,Y_train);\r\n",
        "  pred = rede.predict(X_test);\r\n",
        "  acc = accuracy_score(Y_test, pred);\r\n",
        "  prec = precision_score(Y_test, pred);\r\n",
        "  rec = recall_score(Y_test, pred);\r\n",
        "  f1 = f1_score(Y_test, pred);\r\n",
        "  lista.append(layer)\r\n",
        "  lista.append(acc)\r\n",
        "  lista.append(prec)\r\n",
        "  lista.append(rec)\r\n",
        "  lista.append(f1)\r\n",
        "  df.loc[i] = lista\r\n",
        "  i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQ3GWUbw-TpS",
        "outputId": "46d816c7-9246-40c3-f83f-e0cfc5046216"
      },
      "source": [
        "ativa√ß√µes = ['relu', 'identity', 'logistic', 'tanh']\r\n",
        "layers = [(10,),(20,),(30,),(40,),(50,),(2,2),(3,3),(4,4),(5,5)]\r\n",
        "df = pd.DataFrame(columns=['Ativa√ß√£o','Camadas', 'Acur√°cia', 'Precis√£o', 'Recall', 'F1 Score'])\r\n",
        "i = 0\r\n",
        "for ativa√ß√£o in ativa√ß√µes:\r\n",
        "  i = i\r\n",
        "  for layer in layers:\r\n",
        "    lista = []\r\n",
        "    rede = MLPClassifier(activation=ativa√ß√£o,hidden_layer_sizes=layer,max_iter=300 ,random_state=1);\r\n",
        "    rede.fit(X_train,Y_train);\r\n",
        "    pred = rede.predict(X_test);\r\n",
        "    acc = accuracy_score(Y_test, pred);\r\n",
        "    prec = precision_score(Y_test, pred);\r\n",
        "    rec = recall_score(Y_test, pred);\r\n",
        "    f1 = f1_score(Y_test, pred);\r\n",
        "    lista.append(ativa√ß√£o)\r\n",
        "    lista.append(layer)\r\n",
        "    lista.append(acc)\r\n",
        "    lista.append(prec)\r\n",
        "    lista.append(rec)\r\n",
        "    lista.append(f1)\r\n",
        "    df.loc[i] = lista\r\n",
        "    i = i + 1"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n",
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (300) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pGhlvpbodQd8"
      },
      "source": [
        "\r\n",
        "variar as camadas escondidas\r\n",
        "\r\n",
        "aumentando e diminuindo os neuronios\r\n",
        "\r\n",
        "testar tbm as diferentes ativa√ß√µes\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FbJZi56O_Zm-"
      },
      "source": [
        "df_tanh = df.loc[df['Ativa√ß√£o']=='tanh']\r\n",
        "df_relu = df.loc[df['Ativa√ß√£o']=='relu']\r\n",
        "df_identity = df.loc[df['Ativa√ß√£o']=='identity']\r\n",
        "df_logistic = df.loc[df['Ativa√ß√£o']=='logistic']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHcBj_10LZbG",
        "outputId": "46beeda7-2e2b-4191-c4fe-51c78691db34"
      },
      "source": [
        "df.Acur√°cia.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6923076923076923"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        },
        "id": "V1JSxSsq7_KU",
        "outputId": "a357b82e-c7e2-4722-ead5-8717e22de405"
      },
      "source": [
        "df_relu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Ativa√ß√£o</th>\n",
              "      <th>Camadas</th>\n",
              "      <th>Acur√°cia</th>\n",
              "      <th>Precis√£o</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1 Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>relu</td>\n",
              "      <td>(10,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>relu</td>\n",
              "      <td>(20,)</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.551724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>relu</td>\n",
              "      <td>(30,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.466667</td>\n",
              "      <td>0.538462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>relu</td>\n",
              "      <td>(40,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>relu</td>\n",
              "      <td>(50,)</td>\n",
              "      <td>0.538462</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.400000</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>relu</td>\n",
              "      <td>(2, 2)</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.731707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>relu</td>\n",
              "      <td>(3, 3)</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>0.576923</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.731707</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>relu</td>\n",
              "      <td>(4, 4)</td>\n",
              "      <td>0.692308</td>\n",
              "      <td>0.818182</td>\n",
              "      <td>0.600000</td>\n",
              "      <td>0.692308</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>relu</td>\n",
              "      <td>(5, 5)</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.625000</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.434783</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  Ativa√ß√£o Camadas  Acur√°cia  Precis√£o    Recall  F1 Score\n",
              "0     relu   (10,)  0.538462  0.666667  0.400000  0.500000\n",
              "1     relu   (20,)  0.500000  0.571429  0.533333  0.551724\n",
              "2     relu   (30,)  0.538462  0.636364  0.466667  0.538462\n",
              "3     relu   (40,)  0.538462  0.666667  0.400000  0.500000\n",
              "4     relu   (50,)  0.538462  0.666667  0.400000  0.500000\n",
              "5     relu  (2, 2)  0.576923  0.576923  1.000000  0.731707\n",
              "6     relu  (3, 3)  0.576923  0.576923  1.000000  0.731707\n",
              "7     relu  (4, 4)  0.692308  0.818182  0.600000  0.692308\n",
              "8     relu  (5, 5)  0.500000  0.625000  0.333333  0.434783"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Wo7aLwM7_Nn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60de4c69-cdd5-4e89-f3ab-3346c12dbb79"
      },
      "source": [
        "melhor_rede = MLPClassifier(hidden_layer_sizes=(4,4), max_iter=270 ,activation='relu', random_state=1)\r\n",
        "melhor_rede.fit(X_train, Y_train)\r\n",
        "pred_melhor_rede = melhor_rede.predict(X_test)\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/neural_network/_multilayer_perceptron.py:571: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (270) reached and the optimization hasn't converged yet.\n",
            "  % self.max_iter, ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nrcIpCteHyki",
        "outputId": "6575bb2a-3f7b-4dda-ebf9-172b5e7aa559"
      },
      "source": [
        "confusion_matrix(Y_test, pred_melhor_rede)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[9, 2],\n",
              "       [6, 9]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxh9Gom9pNJc",
        "outputId": "a3c8e90a-20f0-4a41-9f51-3acaf05f1b91"
      },
      "source": [
        "accuracy_score(Y_test,pred_melhor_rede)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6538461538461539"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j8ej9ruhKpbC",
        "outputId": "323fc55a-895c-4166-a000-7c3b523efcf3"
      },
      "source": [
        "Y_test.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    15\n",
              "0    11\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xF7KK33GmHXK",
        "outputId": "ba22c49c-32ad-4c6c-c1a9-7471dda21759"
      },
      "source": [
        "Y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38    0\n",
              "44    1\n",
              "27    0\n",
              "40    0\n",
              "36    0\n",
              "81    1\n",
              "58    1\n",
              "69    1\n",
              "85    1\n",
              "56    1\n",
              "67    1\n",
              "53    1\n",
              "48    1\n",
              "66    1\n",
              "63    1\n",
              "10    0\n",
              "2     0\n",
              "35    0\n",
              "68    1\n",
              "45    1\n",
              "19    0\n",
              "80    1\n",
              "34    0\n",
              "32    0\n",
              "31    0\n",
              "43    1\n",
              "Name: class, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DR-DXrdhAcGw",
        "outputId": "dcb35434-7f52-4eb5-8d4d-e3590f2fba99"
      },
      "source": [
        "df.groupby(by=['Ativa√ß√£o', 'Camadas']).Precis√£o.max().sort_values(ascending=False)[:5]\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ativa√ß√£o  Camadas\n",
              "relu      (4, 4)     0.818182\n",
              "tanh      (50,)      0.666667\n",
              "identity  (4, 4)     0.666667\n",
              "          (10,)      0.666667\n",
              "          (20,)      0.666667\n",
              "Name: Precis√£o, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXcFXPlxDOJI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "624ddc90-c5f0-456d-d278-b10c76681f46"
      },
      "source": [
        "df.groupby(by=['Ativa√ß√£o', 'Camadas']).Acur√°cia.max().sort_values(ascending=False)[:5]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Ativa√ß√£o  Camadas\n",
              "relu      (4, 4)     0.692308\n",
              "          (3, 3)     0.576923\n",
              "logistic  (2, 2)     0.576923\n",
              "relu      (2, 2)     0.576923\n",
              "tanh      (40,)      0.538462\n",
              "Name: Acur√°cia, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "id": "kLCpgoOePHFW",
        "outputId": "4eac7fa2-b48c-4110-88c7-050666c4a4cf"
      },
      "source": [
        "from sklearn.metrics import plot_roc_curve\r\n",
        "plot_roc_curve(melhor_rede, X_test, Y_test)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<sklearn.metrics._plot.roc_curve.RocCurveDisplay at 0x7f4ead4b1ba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUpIpQ8BRgja"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}